{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 470 Activities and Case Studies\n",
    "\n",
    "1. For all activities, you are allowed to collaborate with a partner. \n",
    "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
    "\n",
    "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations with regard to how these notebooks will be graded:\n",
    "\n",
    "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
    "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
    "3. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
    "4. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will lose points for your work in that section.\n",
    "5. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
    "6. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the autograder will ignore the modified \"assert\" statement. Make sure you don't edit the assert statements.\n",
    "7. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
    "8. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
    "9. The **Grading** section at the end of the document (before the **Feedback** section) contains some code for our autograder on GradeScope. You are expected to fail this block of code in your Jupyter environment. DO NOT edit this block of code, or you may not get points for your assignment.\n",
    "10. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33495dc70b269f4040bbecb9b1135ce2",
     "grade": false,
     "grade_id": "cell-c2a67d718e5cbe74",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Unsupervised Learning - Density Estimation\n",
    "\n",
    "In this exercise we'll focus on learning about histograms and how choosing the bin count changes the histogram that is created. From there, we'll look at how changing the density estimation function and its hyperparameters results in better or worse fits to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c3aced8ff424c31b",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import math\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c0ed12f0ba926be",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3fb0a4efb6f9a9b",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "mu_1, sigma_1, n_1 = 0, 0.5, 1000\n",
    "mu_2, sigma_2, n_2 = 2, 1, 500\n",
    "\n",
    "vals_1 = np.random.normal(mu_1, sigma_1, n_1)\n",
    "vals_2 = np.random.normal(mu_2, sigma_2, n_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bd3d02740dcaf515",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "vals = np.concatenate((vals_1, vals_2), axis=0)\n",
    "n = vals.shape[0]\n",
    "sigma = vals.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c9549a418314408",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(vals, 10)\n",
    "plt.title(\"Potentially Misleading Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b2b7bf13f0fcbe8",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1b6823cbf6bfbbef43ab3acbb9ab57c",
     "grade": false,
     "grade_id": "cell-0d06a3a983734f30",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We can calculate the number of bins (bin count), $b$, from the bin width, $w$, as\n",
    "\n",
    "$$\n",
    "b = \\Big\\lceil{\\frac{max - min}{w}}\\Big\\rceil\\\n",
    "$$\n",
    "\n",
    "where $max$ and $min$ are the maximum and minimum value of samples in our data, and $\\lceil{.}\\rceil$ indicates the ceiling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30ffa108db2b08a7ee83c0c1a80c34bd",
     "grade": false,
     "grade_id": "cell-910b2fec64f1b3ee",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def bin_count(max_val, min_val, width):\n",
    "    \"\"\"Return the number of bins for a histogram\n",
    "    \n",
    "    Args:\n",
    "        max_val (float): The maximum value in the dataset\n",
    "        min_val (float): The minimum value in the dataset\n",
    "        width (float): The bin width\n",
    "    \n",
    "    Returns:\n",
    "        int: The number of bins for the histogram\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbca6aa9137104c48825d8f5b59b61e5",
     "grade": true,
     "grade_id": "cell-9123d41096ad1ee0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert bin_count(500, 100, 20) == 20\n",
    "assert bin_count(600, 100, 25) == 20\n",
    "assert bin_count(500, 100, 18) == 23\n",
    "assert isinstance(bin_count(500, 100, 20), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ef24fbf7df204f50d6b41473e19c056",
     "grade": false,
     "grade_id": "cell-d065e8def62a51a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now let's select a better number of bins by using Scott's rule and Freedman & Diaconis' rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3b6fd156889f1907a1f14eeef79ef97",
     "grade": false,
     "grade_id": "cell-25968153c0682214",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def scotts_rule_count(data):\n",
    "    \"\"\"Calculate the number of bins using Scott's rule\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): The data array\n",
    "    \n",
    "    Returns:\n",
    "        int: The number of bins of the histogram\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2f443a7fd3f5fa6929b43d55504e561",
     "grade": true,
     "grade_id": "cell-b9b4813b01b4ece6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_data = np.random.normal(0,1,500)\n",
    "test_data_2 = np.random.uniform(0,5,500)\n",
    "assert isinstance(scotts_rule_count(np.random.normal(0,1,500)), int)\n",
    "assert scotts_rule_count(test_data) in (13,14,15)\n",
    "assert scotts_rule_count(test_data_2) in (8,9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8d88b571e823a16",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "scott_bins = scotts_rule_count(vals)\n",
    "print(f\"Creating a histogram with {scott_bins} bins.\")\n",
    "plt.hist(vals, bins=scott_bins)\n",
    "plt.title(\"Scott's Rule Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f362aad5f62a47b9acb690c772a3dbb",
     "grade": false,
     "grade_id": "cell-eb7a054a424b306a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we'll try Freedman and Diaconis' rule. To calculate the interquartile range, you may want to look into calculating the percentiles using [np.percentile](https://docs.scipy.org/doc/numpy/reference/generated/numpy.percentile.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62c2ca5f58acd9da3d45dcb67b5d296c",
     "grade": false,
     "grade_id": "cell-9213476e5ce4a45d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def fd_rule_count(data):\n",
    "    \"\"\"Calculate the number of bins using Freedman & Diaconis' rule\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): The data array\n",
    "    \n",
    "    Returns:\n",
    "        int: The number of bins for the histogram\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b6013377d923c4992ffdcefd701fda5",
     "grade": true,
     "grade_id": "cell-a6359eff72ee4b15",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert fd_rule_count(test_data) in (15,16,17)\n",
    "assert fd_rule_count(test_data_2) in (8,9,10)\n",
    "assert isinstance(fd_rule_count(test_data), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5264c19f0cbfaaa8",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "fd_bins = fd_rule_count(vals)\n",
    "print(f\"Creating a histogram with {fd_bins} bins.\")\n",
    "plt.hist(vals, bins=fd_bins)\n",
    "plt.title(\"Freedman & Diaconis' Rule Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7f5bcb0677c5128202e46851540dc81",
     "grade": false,
     "grade_id": "cell-92e34c320aba9996",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Kernel Density Estimation\n",
    "\n",
    "Let's add a density estimate to the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4df0f2362e27f5c4",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def kernel_plotter(kernel, data, bins, bandwidth):\n",
    "    plt.hist(data, bins=bins, density=True)\n",
    "    X_plot = np.linspace(data.min(), data.max(), 1000)[:, np.newaxis]\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(data.reshape(-1, 1))\n",
    "    log_dens = kde.score_samples(X_plot)\n",
    "    plt.plot(X_plot[:, 0], np.exp(log_dens))\n",
    "    plt.title(f\"Kernel = '{kernel}', h={bandwidth}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ebe9992d733479b9",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "kernel_plotter(\"linear\", vals, fd_bins, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4edf0cc813d30af104a5abef2b9521ad",
     "grade": false,
     "grade_id": "cell-52c6730dc8c307eb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The tophat, or uniform, kernel will look closest to the actual histogram with sufficient bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2917aca8037182759ffbbba2dce57ee",
     "grade": false,
     "grade_id": "cell-59e1a4bc334f42b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Kernels\n",
    "\n",
    "Now you will plot the various kernels with the same bandwidth value for the data we've been examining. Then you'll plot a variety of densities with the same kernel but different bandwidths. Try out different values and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-71818a37afacf52c",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for kernel in [\"gaussian\",\"tophat\",\"epanechnikov\",\"exponential\",\"linear\",\"cosine\"]:\n",
    "    kernel_plotter(kernel, vals, fd_bins, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandwidth\n",
    "\n",
    "Notice that for a given kernel, lower (narrower) bandwidth results in KDE probability distributions that fluctuate more with the data, and may be noisy. Higher (wider) bandwidth results in smoother distributions with peaks and valleys of lower magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-541a9e14975dbd2e",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for bandwidth in [0.2, 0.5, 0.8, 1, 1.5, 2]:\n",
    "    kernel_plotter(\"tophat\", vals, fd_bins, bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54ba3e8972fa57ff02f66a22250c6db2",
     "grade": false,
     "grade_id": "cell-2c1f22d1e6c1dc8f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Select what you think as the best kernel and best bandwidth\n",
    "# Set them to best_kernel and best_bandwidth respectively\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59f896df0b938a10a16f919e103a4d8d",
     "grade": true,
     "grade_id": "cell-ad9b81ffe7bbfabd",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(best_kernel, str)\n",
    "assert best_kernel in [\"gaussian\",\"tophat\",\"epanechnikov\",\"exponential\",\"linear\",\"cosine\"]\n",
    "assert isinstance(best_bandwidth, float)\n",
    "assert best_bandwidth > 0\n",
    "assert best_bandwidth <= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "512bd5f5ada1640bfe1ecf97ba40b895",
     "grade": false,
     "grade_id": "cell-687d6638954cd5c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Density-Based Clustering\n",
    "\n",
    "Here's the clustering overview from the manifold clustering exercise again. Now most of the remaining clustering methods should make sense as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef7f28d087f7d3aa",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "The following code block is purely used for grading. If you find any error, you can ignore. DO NOT MODIFY THE CODE BLOCK BELOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograding with Otter Grader\n",
    "import otter\n",
    "grader = otter.Notebook()\n",
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb01399b8fdb70164d81285e341eaffe",
     "grade": false,
     "grade_id": "cell-0947a62caa6656ae",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
     "grade": false,
     "grade_id": "feedback",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
     "grade": true,
     "grade_id": "feedback-tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
