{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 470 Activities and Case Studies\n",
    "\n",
    "1. For all activities, you are allowed to collaborate with a partner. \n",
    "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
    "\n",
    "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations with regard to how these notebooks will be graded:\n",
    "\n",
    "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
    "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
    "3. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
    "4. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will lose points for your work in that section.\n",
    "5. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
    "6. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the autograder will ignore the modified \"assert\" statement. Make sure you don't edit the assert statements.\n",
    "7. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
    "8. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
    "9. The **Grading** section at the end of the document (before the **Feedback** section) contains some code for our autograder on GradeScope. You are expected to fail this block of code in your Jupyter environment. DO NOT edit this block of code, or you may not get points for your assignment.\n",
    "10. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "peZlh40I73Ql",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24167670d09bdb61d1a595ce5cbb79c9",
     "grade": false,
     "grade_id": "cell-4fb3a6f3d58fd28b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "In this exercise, you will use a deep neural network to predict the values of houses based on some provided input data. You will use keras to build the model. Below is a description of how the keras models are set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BklNQ52d73Qx",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d07727b5ced87bf2",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOEAeGxU73Qz"
   },
   "outputs": [],
   "source": [
    "house_data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rstw6PPE73Q1",
    "outputId": "16071adb-1b4f-4319-b7ca-b3857a7f0a1e"
   },
   "outputs": [],
   "source": [
    "print(house_data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6yomxx_73Q2",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b0245844d72bf80",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "house_features = pd.DataFrame(house_data[\"data\"], columns=house_data[\"feature_names\"])\n",
    "house_prices = pd.Series(house_data[\"target\"])\n",
    "x_train, x_test, y_train, y_test = train_test_split(house_features, house_prices, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JS1WEyq473Q2",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e2de7844f2c7f49e",
     "locked": false,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "c3dd00a5-8855-4564-c79e-b819d94822e5"
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDDMD-fc73Q2"
   },
   "source": [
    "This tells us that there are 16512 samples with 8 features each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJUWPv5M73Q3",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33b45d559f48738f",
     "locked": false,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "6f5432c9-f09b-4cfa-9278-3cbb4dce0bc9"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zw7tYjwQ73Q3"
   },
   "source": [
    "This tells us that there are 16512 samples of a single target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4LXZO6b73Q4",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-831e0268417ab95a",
     "locked": false,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "c9476dac-551b-45c4-ee78-e207ab056a43"
   },
   "outputs": [],
   "source": [
    "y_train.mean(), y_train.std(), y_train.min(), y_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4RNTghM_73Q4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7c48fef23f2aa185655ff27a0f3859a",
     "grade": false,
     "grade_id": "cell-2f0fe3b3a560d952",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The keras model consists of multiple parts:\n",
    "\n",
    "1. Construct the model layers, neurons per layer, and activation functions\n",
    "1. Determine the loss function, metrics, and optimization method\n",
    "1. Fit the model to some data\n",
    "1. Evaluate the model using the same metric\n",
    "\n",
    "Some relevant docs:\n",
    " - [initializers](https://keras.io/initializers/)\n",
    " - [loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    " - [regularizations](https://keras.io/regularizers/)\n",
    " - [optimizers](https://keras.io/optimizers/)\n",
    " - [metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5uuYQoLp73Q4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf844c11123af1ac74291ba99ac72d3e",
     "grade": false,
     "grade_id": "cell-cae21934fa064b47",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "First, to construct a model, use the [Sequential](https://keras.io/getting-started/sequential-model-guide/) object. You can pass a list of layers to the sequential object. For this exercise, we will only be using the [Dense](https://keras.io/layers/core/#dense) layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "K6PeTXhf73Q7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d2aac7a6df929de0a6c49c73b3aea00",
     "grade": false,
     "grade_id": "cell-51879de7032d8823",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of layers with the variable name \"layers\".\n",
    "#\n",
    "# - The list should contain 3 consecutive hidden layers with 100 neurons each,\n",
    "#   and an output layer with 1 neuron. \n",
    "# - In the first layer, you should specify an input_shape. It is not necessary to do so\n",
    "#   in subsequent layers.\n",
    "# - The output layer should have 1 neuron because we are predicting a single regression target.\n",
    "# - Use any activation you like such as \"relu\" or \"tanh\", you can alternate for each layer\n",
    "#   For your first run, try using the linear activation and then see if modifying the activations\n",
    "#   improves the result.\n",
    "\n",
    "# Pass the list to keras.Sequential and save the returned model to the variable name \"model\".\n",
    "\n",
    "# Optional - give each layer a name and see how that shows up in the model summary.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_xI-mvPz73Q8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdcadadcfa2f93a7140b16760cc8c79a",
     "grade": true,
     "grade_id": "cell-ee74eff075f222ed",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(model, Sequential)\n",
    "assert len(layers) == 4\n",
    "for i, layer in enumerate(layers):\n",
    "    assert isinstance(layers[i], Dense)\n",
    "    assert layer.weights[1].shape == [100, 100, 100, 1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET37Z5F273Q8"
   },
   "source": [
    "In TensorFlow, models are \"compiled\" before training. Compiling specifies the type of optimizer (e.g., gradient descent) and loss function, and creates code that will run efficiently on your hardware during training and model prediction.\n",
    "\n",
    "Review the model's `compile()` method, and use it to create compiled code in the cell below.  \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\n",
    "\n",
    "In this notebook we'll use the [stochastic gradient descent (SGD)](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) and the [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "M1HSCPWV73Q8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2964c3f063261c63a62d89a699e3c69",
     "grade": false,
     "grade_id": "cell-9c5dcca57115d02a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# - Instantiate an SGD optimizer with learning rate of 1e-7, and save it as \"optimizer\"\n",
    "# - Apply the model's .compile() method to set it up for training.\n",
    "#   - Note that you can use each loss and metric's string name rather\n",
    "#     than instantiating a loss or metric object, and passing the object.\n",
    "#     Because we want to set a non-defaul learning rate, we must\n",
    "#     instantiate an optimizer object and pass that to compile(), rather\n",
    "#     than an optimizer string name.\n",
    "\n",
    "# Set up the model to do the following:\n",
    "# - use stochastic gradient descent (sgd) to fit the model (via your 'optimizer' object)\n",
    "# - use mean absolute error (mae) as its *loss function*\n",
    "# - use mean absolute error (mae) as one of its *metrics*\n",
    "# - use mean squared error (mae) as one of its *metrics*\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rPDkIZNE73RA",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7834482fb4dcda0fbfd736e59ef85ab7",
     "grade": true,
     "grade_id": "cell-0746e29c34118826",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(model.optimizer, keras.optimizers.SGD)\n",
    "assert model.loss in [\"mae\", \"mean_absolute_error\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the model for 50 epochs with a batch size of 1000. This means the following:\n",
    "\n",
    "- The training set will be split into batches, each composed of 1000 samples.\n",
    "- Each batch will be processed by the neural network to produce 1000 predications.\n",
    "- The predictions will be compared against the targets to compute the losses.\n",
    "- The loss values are passed backwards through the network to compute the loss gradients for those 1000 samples.\n",
    "- Via the SGD procedure, the averages of those 1000 gradients are used to update the model's parameters.\n",
    "- After all batches have been processed in this manner, one \"epoch\" has been completed.\n",
    "- This is done again and again until the number of specified epochs (50) are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHtGEIqL73RE",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8bbd77fa779d371",
     "locked": false,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "dd978455-1a58-4ba2-c1b8-a0d506c08be2"
   },
   "outputs": [],
   "source": [
    "# Now fit the model\n",
    "n_epochs = 20\n",
    "history = model.fit(x_train, y_train, batch_size=1000, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had you set the learning rate to a very small value, so you could seed the iterative improvement across training epochs. More typicaly learning rates are 1e-3 or 1e-4, but the housing data is so \"simple\" relative to the power of our neural network, we would have had pretty good convergence within a single epoch, and you would not have been able to observe the more typical, steady progress to convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the training set loss as a function of training epoch, as well\n",
    "# as the metric scores. In general, if you don't see your loss curve flatten\n",
    "# at the end of the training session, you'l want to increase the number of\n",
    "# training epochs, or increase the learning rate.\n",
    "#\n",
    "# In this case, if it doesn't look nearly flat than you've likely specified\n",
    "# your model or learning rate incorrectly.\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(1, n_epochs+1), history.history['loss'])\n",
    "plt.title('Training set loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(np.arange(1, n_epochs+1), history.history['mae'], label='mae')\n",
    "plt.semilogy(np.arange(1, n_epochs+1), history.history['mse'], label='mse')\n",
    "plt.legend()\n",
    "plt.title('Training set metric scores')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "print(f\"Training loss on the final epoch was: {history.history['loss'][-1]:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-9dv4rc73RE",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d280c00c6cabc586",
     "locked": false,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "eabd296a-771e-485c-b912-2839e0c10fad"
   },
   "outputs": [],
   "source": [
    "# Here we can evaluate how our model does based on the test data\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HjQ6dSYs73RF",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "326bc26a2d582914d7afc0a6aef3f52a",
     "grade": false,
     "grade_id": "cell-f88eeeebe2137b4d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now let's try another optimizer instead of stochastic gradient descent (SGD). [Adam](https://keras.io/optimizers/#adam) is the recommended default for training neural networks since it usually performs quite well. In the next cell, compile the model with Adam instead of SGD and use the same loss and metrics. After compiling, fit the data for as many epochs as you think it takes to see the value start to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "tbkjvYLx73RF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31fc47c0632aae2847e9b28ad900c2fe",
     "grade": false,
     "grade_id": "cell-bee688f3d919be61",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "137f80d5-78fa-4648-810d-9b1a87173888"
   },
   "outputs": [],
   "source": [
    "# - Instantiate an Adam optimizer with learning rate of 1e-5, and save it as \"optimizer\"\n",
    "# - Recompile the model using Adam and the same loss and metrics as previously\n",
    "# - Call .fit() to train you model, using a batch size of 1000. You choose the number of epochs.\n",
    "# - Note that we are now using a larger learning rate, so convergence\n",
    "#   should occur more quickly.\n",
    "\n",
    "# Before starting, we'll reset the model parameters back to their original,\n",
    "# random state, so we aren't trying to train an already trained model.\n",
    "model.set_weights(initial_weights)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "p1qfXNGA73RG",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "526079ba3785fa572cc21eb9fd5fb7af",
     "grade": true,
     "grade_id": "cell-fe8cd91b8b780964",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(model.optimizer, keras.optimizers.Adam)\n",
    "assert model.loss in [\"mae\", \"mean_absolute_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_t7W2xD73RG"
   },
   "outputs": [],
   "source": [
    "# You can optionally visualize the model here, with some added package installs.\n",
    "\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the training loss and metric scores again\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(1, n_epochs+1), history.history['loss'])\n",
    "plt.title('Training set loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(np.arange(1, n_epochs+1), history.history['mae'], label='mae')\n",
    "plt.semilogy(np.arange(1, n_epochs+1), history.history['mse'], label='mse')\n",
    "plt.legend()\n",
    "plt.title('Training set metric scores')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "print(f\"Training loss on the final epoch was: {history.history['loss'][-1]:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3zBxrMk73RG",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5b63d6e873e152e",
     "locked": false,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "d5a572f5-3921-4513-c566-45bf50e1e0bd"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3k88_NmP73RH",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d16731044b4c469ddea89069b3100569",
     "grade": false,
     "grade_id": "cell-9ce06e7b86737575",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now recreate the model, again named `model`, with dropout layers. Add two dropout layers, using [`Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), one after the first layer of neurons and one after the second layer of neurons.\n",
    "Create a new list of layers, and name it `d_layers`, rather than reusing your old list (so that the model parameters will be randomly initialized again). Then construct the model as before.\n",
    "Select a low value of dropout (say, <0.1) that results in a good score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "HUy6UJp273RH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b57f0598104624eee5418f3a538acac",
     "grade": false,
     "grade_id": "cell-d5040e44398f8dc9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create your new model, with dropout layers.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LeG72RM73RO",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6179f8d29fe18a67",
     "locked": false,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "99a771bd-f2af-486e-8672-e40659d82ed3"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MtrNxcUo73RO",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1112b38e98fd56d9af81ced88c256d10",
     "grade": true,
     "grade_id": "cell-b6c7e6e2ddc6742a",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(d_layers) == 6\n",
    "assert isinstance(d_layers[1], Dropout)\n",
    "assert isinstance(d_layers[3], Dropout)\n",
    "for i,layer in enumerate(layers):\n",
    "    if i not in [1,3]:\n",
    "        assert isinstance(d_layers[i], keras.layers.Dense) \n",
    "        assert layer.weights[1].shape == [100, 0, 100, 0, 100, 1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LVaKsFK73RO",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ed3ec168ee4a1e5",
     "locked": false,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "a6aab0e6-455b-4ca5-81c0-d5efa3f8a40e"
   },
   "outputs": [],
   "source": [
    "# Because dropout reduces the convergence rate (but may ultimately coverge to\n",
    "# a lower loss), it sometimes requires training for more epochs than otherwise.\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss='mae', metrics=['mae', \"mse\"])\n",
    "n_epochs = 50\n",
    "history = model.fit(x_train, y_train, batch_size=1000, epochs=n_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the training loss and metric scores again\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(1, n_epochs+1), history.history['loss'])\n",
    "plt.title('Training set loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(np.arange(1, n_epochs+1), history.history['mae'], label='mae')\n",
    "plt.semilogy(np.arange(1, n_epochs+1), history.history['mse'], label='mse')\n",
    "plt.legend()\n",
    "plt.title('Training set metric scores')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "print(f\"Training loss on the final epoch was: {history.history['loss'][-1]:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LUex7SPp73RO",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f892cfea01ec6c2301d8cf6df12f2bfc",
     "grade": false,
     "grade_id": "cell-06909b3cd0248748",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Select a dropout rate that gets an okay score--one that is sufficient to pass the following `assert` test.\n",
    "\n",
    "Note that in this case, dropout did not likely result in better loss or metric scores on the test set versus the models without dropout. Dropout does not always help, but it sometimes does, especially if you have a very large model but a somewhat undersized dataset. In that case, dropout serves as a regularization technique, like L1- or L2-norm weight penalty terms in linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "QD8pzPFe73RO",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1f1a9bcfa2ab8ad1ed3b21b00fb8ba9",
     "grade": true,
     "grade_id": "cell-538bede4947fe364",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "9b2ea88e-427a-4173-a969-9595722ff767"
   },
   "outputs": [],
   "source": [
    "assert model.evaluate(x_test, y_test)[0] < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "The following code block is purely used for grading. If you find any error, you can ignore. DO NOT MODIFY THE CODE BLOCK BELOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograding with Otter Grader\n",
    "import otter\n",
    "grader = otter.Notebook()\n",
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IPWJ3CQq73RP",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23648526b4581cb6ea5df76b63c45274",
     "grade": false,
     "grade_id": "cell-bdd8daf85b096db9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "8ggmw34M73RP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
     "grade": false,
     "grade_id": "feedback",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "l3-dgeRp73RP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
     "grade": true,
     "grade_id": "feedback-tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5a_Deep_Learning_Alternate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
