{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZILWnYt0x_aK"
      },
      "source": [
        "## CSCI 470 Activities and Case Studies\n",
        "\n",
        "1. For all activities, you are allowed to collaborate with a partner.\n",
        "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
        "\n",
        "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRyCddsfx_aL"
      },
      "source": [
        "Some considerations with regard to how these notebooks will be graded:\n",
        "\n",
        "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
        "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**.\n",
        "3. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
        "4. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will lose points for your work in that section.\n",
        "5. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
        "6. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the autograder will ignore the modified \"assert\" statement. Make sure you don't edit the assert statements.\n",
        "7. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
        "8. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
        "9. The **Grading** section at the end of the document (before the **Feedback** section) contains some code for our autograder on GradeScope. You are expected to fail this block of code in your Jupyter environment. DO NOT edit this block of code, or you may not get points for your assignment.\n",
        "10. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f3422b5f55e0ed438ce80616d6b6e2c4",
          "grade": false,
          "grade_id": "cell-d07e9bbbd9efabf6",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "EArSMKLIx_aL"
      },
      "source": [
        "# Deep Learning - Recurrent Neural Networks\n",
        "\n",
        "## NLP for sentiment analysis\n",
        "\n",
        "### Important...\n",
        "Depending on the size of your models and the number of epochs you use for training, this notebook could have lengthy runtimes. __You are encouraged to initially use at least 3 epochs of training for each model, so you can observe how well the model is or is not converging, and whether or not overfitting is occurring.__ You may even want to train for 10+ epochs and let the notebook run while you are doing something else, or overnight, to observe how many training epochs it takes for models to converge to their asypmtotes (loosely speaking).\n",
        "\n",
        "__Prior to submittal, set the number of training epochs equal to 1, for all three models__. Otherwise the grading cannot be completed in a reasonable amount of time. __There is a test cell at the end of the notebook that checks for this.__ Thanks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU502Nzqx_aL"
      },
      "source": [
        "In this notebook you do not need to find hyperparameters that result in best model performance. The intent is for you to run and re-run the notebook a few times with different hyperparameters, and observe how that impacts performance (test set scores), generally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-aaad388f467a51c0",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "kGACw5MZx_aM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Embedding, Dense\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8bb683b4db83a419420a6de2547a4ad4",
          "grade": false,
          "grade_id": "cell-bda9bc8e1d0c997d",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "DxJdhTW_x_aM"
      },
      "source": [
        "We will be using the IMDB dataset outlined in the keras documentation [here](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification). We will be applying a supervised learning model to IMDB movie reviews (text) and predict sentiment based on that text.\n",
        "\n",
        "Take a look at the `import`s above. For the RNN-based imports see the [RNN documentation](https://keras.io/layers/recurrent). For preprocessing using `sequence` see the [sequence documentation](https://keras.io/preprocessing/sequence). For Embedding, see the [Embedding documentation](https://keras.io/layers/embeddings/).\n",
        "\n",
        "From the Keras documentation, linked above:\n",
        ">\"This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c5683134993794d6",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D0gDZ1ax_aM",
        "outputId": "e523813b-fcb8-4649-b709-8155f3b2b7cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Download the IMDB data, to our specifications\n",
        "\n",
        "n = 20000 # Only use the most frequent n words\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wK4G_7Px_aN"
      },
      "source": [
        "If you get a __VisibleDeprecationWarning__ in the cell above, you can ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2d04aa072205ee4a",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKsUEvOXx_aN",
        "outputId": "b28f2538-5295-4611-b079-dd38c0d6b898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training set has 25000 samples.\n",
            "The test set has 25000 samples.\n"
          ]
        }
      ],
      "source": [
        "# How many samples do we have?\n",
        "\n",
        "print(f'The training set has {len(x_train)} samples.')\n",
        "print(f'The test set has {len(x_test)} samples.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5sndEJ_x_aN"
      },
      "source": [
        "## Text Representation\n",
        "\n",
        "The IMDB data has already been converted from text to a sequence of numbers (indices), with each number representing an individual word (most likely, the words were lemmatized before conversion to index numbers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dOR6Y5Qx_aO",
        "outputId": "2a1cf23c-2a56-453d-f6e1-b175913175be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sample 0 has 218 items: \n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "\n",
            "Training sample 1 has 189 items: \n",
            "[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
            "\n",
            "Training sample 2 has 141 items: \n",
            "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# What do some of the individual samples looks like?\n",
        "\n",
        "for i in range(3):\n",
        "    print(f'Training sample {i} has {len(x_train[i])} items: ')\n",
        "    print(x_train[i])\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8dUTFPjx_aO",
        "outputId": "97ec5fce-360f-46c4-fa20-eebf695b4676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set range of indices:\n",
            "1\n",
            "19972\n",
            "\n",
            "Test set range of indices:\n",
            "1\n",
            "19962\n"
          ]
        }
      ],
      "source": [
        "# What are the lowest and highest indices in our training and test features?\n",
        "\n",
        "# The data are numpy arrays, with each item in the array being a list. So\n",
        "# we need to call min (max) twice to get the global min (max).\n",
        "\n",
        "print('Training set range of indices:')\n",
        "print(np.min(np.min(x_train)))\n",
        "print(np.max(np.max(x_train)))\n",
        "\n",
        "print('\\nTest set range of indices:')\n",
        "print(np.min(np.min(x_test)))\n",
        "print(np.max(np.max(x_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtU5UT7bx_aO"
      },
      "source": [
        "As expected, no index is greater than 20,000, our specified maximum number of the most frequenct words we'll allow in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-59607cac09a99a93",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otx-tYuox_aO",
        "outputId": "fb0f36bd-81fe-48e3-9e56-3f9221a737ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sample 0 has a length of 218\n",
            "Training sample 1 has a length of 189\n",
            "Training sample 2 has a length of 141\n",
            "Training sample 3 has a length of 550\n",
            "Training sample 4 has a length of 147\n",
            "Training sample 5 has a length of 43\n",
            "Training sample 6 has a length of 123\n",
            "Training sample 7 has a length of 562\n",
            "Training sample 8 has a length of 233\n",
            "Training sample 9 has a length of 130\n"
          ]
        }
      ],
      "source": [
        "# Reviews are not of a fixed length. Let's see how long a few more of them are.\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"Training sample {i} has a length of {len(x_train[i])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-29008f854c9ed905",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "lLMcLD8dx_aO"
      },
      "outputs": [],
      "source": [
        "# Rather than process variable-length sequences (reviews), we'll\n",
        "# pad sequences that are shorter than our chosen length, and trim\n",
        "# off the end of sequences that are longer than that length.\n",
        "# The padding value is 0, which is not in the vocabulary of indices\n",
        "# given to us in the IMDB dataset (e.g., 0 does not represent/index\n",
        "# a word).\n",
        "\n",
        "maxlen = 100  # Pad or trim sequences to this length\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-26b3b6b877561751",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfrs7TG8x_aO",
        "outputId": "f33efe01-44ea-4b86-f103-82038173a616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 100)\n",
            "(25000, 100)\n"
          ]
        }
      ],
      "source": [
        "# Let's confirm that our samples are now of the fixed, desired length.\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9c1b0c9c7d26e3fc",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeB8t6svx_aO",
        "outputId": "5cbb5d0c-3077-484c-b604-7597e25bffdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# What do our targets look like?\n",
        "\n",
        "np.unique(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "675ccf0c42494c0f840270408969a7a2",
          "grade": false,
          "grade_id": "cell-80d546e03cc3a91a",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "MEys9Orjx_aP"
      },
      "source": [
        "## Text Representation\n",
        "\n",
        "Again, each data sample is a sequence of integers that represent the index of the word in our vocabulary. This saves on storage when compared to a vector that's as long as our vocabulary with all 0's and just one 1 (\"one-hot\" coding).\n",
        "\n",
        "We will be using the [Embedding layer](https://keras.io/layers/embeddings/) as the front-end of our neural network. The Embedding layer serves the same purpose of the Word2Vec and GloVe word representations we discussed in class. It converts the high-dimensional index-based representations (the length of that dimension being equal to the vocabulary size) to a lower-dimensional representation that is better suited for the RNN network that follows. In Word2Vec and GloVe this was done via a self-supervised learning approach (remember the skip-grams?). In our model, the embedding will be learned jointly along with the task (sentiment prediction) via supervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4bde75a095337273ab9f9936ef51e625",
          "grade": false,
          "grade_id": "cell-a96be599550f72f2",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "mBZP3bAkx_aP"
      },
      "source": [
        "## Our models\n",
        "\n",
        "We will build three networks, each using one of three different basic Recurrent Neural Network modules\n",
        "1. Basic/vanilla \"Simple\" RNN modules\n",
        "2. GRU (Gated Recurrent Unit) modules\n",
        "3. LSTM (Long Short-Term Memory) modules\n",
        "\n",
        "We will then compare their performance in predicting the binary sentiment classes of reviews.\n",
        "\n",
        "Note that all each of the RNN modules uses a hyperbolic tangent activation, by default. This activation function constrains the output to be between -1 and 1. Due the the feedback nature of RNNs, if the activations are not contrained to this range, the outputs may eventually \"blow-up\" (increase until there is a numeric overflow). We'll stick with the use of that activation function. This can make learning hard, however, due to the shallow gradients imposed by the hyperbolic tangent. The LSTM and GRU modules were designed to help overcome this shallow gradient problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f906037ac6b3897c6e3bd1da73b5d1cc",
          "grade": false,
          "grade_id": "cell-89247ca5db4b603e",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0IUWYpOx_aP",
        "outputId": "94d66f85-989b-40d6-d480-fb060bc60b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "## Create your Simple RNN\n",
        "#\n",
        "# Define \"layers_simple\", a list of Keras layers, that you will then use to create a Sequential model\n",
        "# saved as \"rnn_simple\".\n",
        "#\n",
        "# Here you will create a simple RNN using one SimpleRNN layer with dropout and\n",
        "# recurrent_dropout (see argument options in SimpleRNN documentation).\n",
        "# Set the dropout rates to 0.2 to start with. After your first run, you may want to\n",
        "# adjust the rates (see instructional guidance in subsequent cells).\n",
        "#\n",
        "# You will need to use an Embedding layer as the first layer (to convert from 20,000 word categories\n",
        "# to a lower dimensionality) followed by the SimpleRNN layer. Select an embedding size (number of\n",
        "# neurons) of your choice, and use that for your SimpleRNN layer's output size as well.\n",
        "# A suggesting starting range is 64 to 128 (the lower it is, the faster this notebook\n",
        "# will run).\n",
        "#\n",
        "# Finally, create an output Dense layer with a single neuron, and an appropriate\n",
        "# activation function for a binary classification task.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "embedding_dim = 64\n",
        "\n",
        "layers_simple = [\n",
        "    Embedding(n, embedding_dim, input_length=maxlen),\n",
        "    SimpleRNN(embedding_dim, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "]\n",
        "\n",
        "rnn_simple = Sequential(layers_simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "28ab0ec9e08974c3cd0d1d3463871bda",
          "grade": true,
          "grade_id": "cell-a693cb84784a5e8d",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "f2wcrWI0x_aP"
      },
      "outputs": [],
      "source": [
        "assert len(layers_simple) == 3\n",
        "assert isinstance(layers_simple[0], Embedding)\n",
        "assert isinstance(layers_simple[1], SimpleRNN)\n",
        "assert isinstance(layers_simple[2], Dense)\n",
        "assert layers_simple[0].output_dim == layers_simple[1].units\n",
        "assert layers_simple[1].dropout > 0\n",
        "assert layers_simple[1].recurrent_dropout > 0\n",
        "assert rnn_simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-10a0d33cc462aa67",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "aLNGNvRLx_aP",
        "outputId": "340e168f-3ab4-4869-aedc-7eff63ca7703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 47ms/step - accuracy: 0.5157 - loss: 0.7064 - val_accuracy: 0.5552 - val_loss: 0.6743\n",
            "Epoch 2/3\n",
            "\u001b[1m 16/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.5683 - loss: 0.6691"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-90a6aded5ad9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrnn_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m history_simple = rnn_simple.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m     14\u001b[0m                                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 batch_size=32, epochs=n_epochs)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the Simple RNN model.\n",
        "# Compute metrics for the test set as well, after each training epoch.\n",
        "\n",
        "# This may take a while. Adjust the n_epochs as you see fit, observing the\n",
        "# convergence of the loss score in the plot created in the cell further below.\n",
        "\n",
        "# IMPORTANT!!\n",
        "# Set n_epochs to 3 or larger on your initial run!\n",
        "# Set n_epochs to 1 for your final notebook run, before submission!\n",
        "n_epochs = 3\n",
        "\n",
        "rnn_simple.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_simple = rnn_simple.fit(x_train, y_train,\n",
        "                                validation_data=(x_test, y_test),\n",
        "                                batch_size=32, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouy_9AZgx_aP"
      },
      "source": [
        "## Is your Simple RNN model overfitting?\n",
        "\n",
        "Examine the plot below. Near the end of training (later epochs), is the training set loss getting lower but the test set loss getting higher? If so, you model is overfitting to the training data. __You may not be able to completely prevent it, but you can try to reduce the problem by__:\n",
        "1. Reducing the number of neurons in your SimpleRNN layer. This reduces the overall number of model parameters, which helps to combat overfitting.\n",
        "2. Increase your dropout rates (particularly the recurrent dropout), which increases regularization.\n",
        "\n",
        "Too much dropout (especially `dropout`, rather than `recurrent_dropout`) may actually hinder performance, on both training and test sets. Choosing dropout rates can be a bit of a guessing game, but we use scores from the test set to guide us (if we had a lot of compute time available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZS4hyfIx_aP"
      },
      "outputs": [],
      "source": [
        "# Plot the loss and accuracy as a function of training epoch\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.arange(1, n_epochs+1), history_simple.history['loss'], '-o', label='Train')\n",
        "plt.plot(np.arange(1, n_epochs+1), history_simple.history['val_loss'], '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.arange(1, n_epochs+1), history_simple.history['accuracy'], '-o', label='Train')\n",
        "plt.plot(np.arange(1, n_epochs+1), history_simple.history['val_accuracy'], '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "81df01c32e6a60d832c2d4bd31efd947",
          "grade": false,
          "grade_id": "cell-9b7b7778053b0aa3",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "Wh8lQTc5x_aQ"
      },
      "outputs": [],
      "source": [
        "## Create your GRU RNN\n",
        "#\n",
        "# Define \"layers_gru\", a list of Keras layers, that you will then use to create a\n",
        "# Sequential model saved as \"rnn_gru\".\n",
        "#\n",
        "# Here you will create an RNN using a GRU layer, with dropout and recurrent_dropout.\n",
        "#\n",
        "# Use an input Embedding layer and output Dense layer, as in the simple RNN model.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "embedding_dim = 64\n",
        "\n",
        "layersq_gru = [\n",
        "    Embedding(n, embedding_dim, input_length=maxlen),\n",
        "    GRU(embedding_dim, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "]\n",
        "\n",
        "rnn_gru = Sequential(layers_gru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f76998f0f57c75db8f5d752c617477e3",
          "grade": true,
          "grade_id": "cell-c69132493de5ac88",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "_zdLrspQx_aQ"
      },
      "outputs": [],
      "source": [
        "assert len(layers_gru) == 3\n",
        "assert isinstance(layers_gru[0], Embedding)\n",
        "assert isinstance(layers_gru[1], GRU)\n",
        "assert isinstance(layers_gru[2], Dense)\n",
        "assert layers_gru[0].output_dim == layers_gru[1].units\n",
        "assert layers_gru[1].dropout > 0\n",
        "assert layers_gru[1].recurrent_dropout > 0\n",
        "assert rnn_gru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-5c71db6dcb81eb2a",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "SlgFp-OUx_aQ"
      },
      "outputs": [],
      "source": [
        "# Train the GRU RNN model.\n",
        "# Compute metrics for the test set as well, after each training epoch.\n",
        "\n",
        "# This may take a while. Adjust the n_epochs as you see fit, observing the\n",
        "# convergence of the loss score in the plot created in the cell further below.\n",
        "\n",
        "# IMPORTANT!!\n",
        "# Set n_epochs to 3 or larger on your initial run!\n",
        "# Set n_epochs to 1 for your final notebook run, before submission!\n",
        "n_epochs = 3\n",
        "\n",
        "rnn_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_gru = rnn_gru.fit(x_train, y_train,\n",
        "                          validation_data=(x_test, y_test),\n",
        "                          batch_size=32, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqfd2-QOx_aQ"
      },
      "source": [
        "## Is your GRU RNN model overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbn2Wj66x_aQ"
      },
      "outputs": [],
      "source": [
        "# Plot the loss and accuracy as a function of training epoch\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.arange(1, n_epochs+1), history_gru.history['loss'], '-o', label='Train')\n",
        "plt.plot(np.arange(1, n_epochs+1), history_gru.history['val_loss'], '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.arange(1, n_epochs+1), history_gru.history['accuracy'], '-o', label='Train')\n",
        "plt.plot(np.arange(1, n_epochs+1), history_gru.history['val_accuracy'], '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "316a878bee0fd96ed1fa799c8482e6db",
          "grade": false,
          "grade_id": "cell-798617aedf043516",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "oqkmA6Olx_aQ"
      },
      "outputs": [],
      "source": [
        "## Create your LSTM RNN\n",
        "#\n",
        "# Define \"layers_lstm\", a list of Keras layers, that you will then use to create a Sequential model\n",
        "# saved as \"rnn_lstm\".\n",
        "#\n",
        "# Here you will create an RNN using an LSTM layer, again, with dropout and recurrent_dropout.\n",
        "#\n",
        "# Use an input Embedding layer and output Dense layer, as in the simple RNN and the GRU model.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "embedding_dim = 64\n",
        "\n",
        "layers_lstm = [\n",
        "    Embedding(n, embedding_dim, input_length=maxlen),\n",
        "    LSTM(embedding_dim, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "]\n",
        "\n",
        "rnn_lstm = Sequential(layers_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ad8d3da4e85d49f56c708a29b928966e",
          "grade": true,
          "grade_id": "cell-58a52ce9bceabdee",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "tWcJkbg5x_aQ"
      },
      "outputs": [],
      "source": [
        "assert len(layers_lstm) == 3\n",
        "assert isinstance(layers_lstm[0], Embedding)\n",
        "assert isinstance(layers_lstm[1], LSTM)\n",
        "assert isinstance(layers_lstm[2], Dense)\n",
        "assert layers_lstm[0].output_dim == layers_lstm[1].units\n",
        "assert layers_lstm[1].dropout > 0\n",
        "assert layers_lstm[1].recurrent_dropout > 0\n",
        "assert rnn_lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-33238b0da6c68f44",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "pUiEGPDrx_aQ"
      },
      "outputs": [],
      "source": [
        "# Train the LSTM RNN model.\n",
        "# Compute metrics for the test set as well, after each training epoch.\n",
        "\n",
        "# This may take a while. Adjust the n_epochs as you see fit, observing the\n",
        "# convergence of the loss score in the plot created in the cell further below.\n",
        "\n",
        "# IMPORTANT!!\n",
        "# Set n_epochs to 3 or larger on your initial run!\n",
        "# Set n_epochs to 1 for your final notebook run, before submission!\n",
        "n_epochs = 3\n",
        "\n",
        "rnn_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_lstm = rnn_lstm.fit(x_train, y_train,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            batch_size=32, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nvAMBrKx_aQ"
      },
      "source": [
        "## Is your LSTM RNN model overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qo5Y65_x_aR"
      },
      "outputs": [],
      "source": [
        "# Plot the loss and accuracy as a function of training epoch\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.arange(1, n_epochs+1), history_lstm.history['loss'], '-o', label='Train')\n",
        "plt.plot(np.arange(1, n_epochs+1), history_lstm.history['val_loss'], '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.arange(1, n_epochs+1), history_lstm.history['accuracy'], '-o', label='Train')\n",
        "plt.plot(np.arange(1, n_epochs+1), history_lstm.history['val_accuracy'], '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a6d552f16291d6329dc6e258f958421a",
          "grade": false,
          "grade_id": "cell-43b2247e569276bc",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "0034XnASx_aR"
      },
      "outputs": [],
      "source": [
        "# Apply the Model.evaluate() methods of your models to the test set and save the\n",
        "# returned losses and accuracies to the corresponding variable names:\n",
        "#   loss_simple, loss_gru, loss_lstm\n",
        "#   acc_simple, acc_gru, acc_lstm\n",
        "#\n",
        "# Note that these values should be very close to the losses/accuracies in the\n",
        "# plots created above, for the final epoch.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "loss_simple, acc_simple = rnn_simple.evaluate(x_test, y_test)\n",
        "loss_gru, acc_gru = rnn_gru.evaluate(x_test, y_test)\n",
        "loss_lstm, acc_lstm = rnn_lstm.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-24347394026d518c",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "PTM3UntJx_aR"
      },
      "outputs": [],
      "source": [
        "print(f\"Your simple model achieved an accuracy of {acc_simple:.2}.\")\n",
        "print(f\"Your GRU model achieved an accuracy of {acc_gru:.2}.\")\n",
        "print(f\"Your LSTM model achieved an accuracy of {acc_lstm:.2}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "10afc519ba24ceed6c3770950a16c1b6",
          "grade": false,
          "grade_id": "cell-95c1830b360ac2b2",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "llUlNJ61x_aR"
      },
      "source": [
        "### Model architecture choices\n",
        "\n",
        "Note that we created models with only __one RNN layer__. We could likely achieve better results by adding more RNN layers but the model would take a much longer time to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6bd06860278e53b54b405d4a39682b78",
          "grade": true,
          "grade_id": "cell-3e1e2805ce275b4c",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "CbRShyxSx_aR"
      },
      "outputs": [],
      "source": [
        "assert acc_simple > 0.4\n",
        "assert acc_gru > 0.6\n",
        "assert acc_lstm > 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1ff557b54c29795b6d3669b57cbe6919",
          "grade": true,
          "grade_id": "cell-a7e003331157fffb",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "dPokf3wMx_aR"
      },
      "outputs": [],
      "source": [
        "# These test are used to confirm that your final,\n",
        "# submitted notebook used 1 and only 1 training\n",
        "# epoch for each model.\n",
        "\n",
        "assert len(history_simple.history['loss'])==1  # confirm that only one epoch of training was done\n",
        "assert len(history_gru.history['loss'])==1  # confirm that only one epoch of training was done\n",
        "assert len(history_lstm.history['loss'])==1  # confirm that only one epoch of training was done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6-dTVjYx_aR"
      },
      "source": [
        "# Grading\n",
        "The following code block is purely used for grading. If you find any error, you can ignore. DO NOT MODIFY THE CODE BLOCK BELOW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIW307x7x_aR"
      },
      "outputs": [],
      "source": [
        "# Autograding with Otter Grader\n",
        "import otter\n",
        "grader = otter.Notebook()\n",
        "grader.check_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "830573f46459d11e5238532d8759463e",
          "grade": false,
          "grade_id": "cell-bc503c7f94e3b8bf",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "ogOAY2pKx_aR"
      },
      "source": [
        "## Feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
          "grade": false,
          "grade_id": "feedback",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "izxF9Oxkx_aR"
      },
      "outputs": [],
      "source": [
        "def feedback():\n",
        "    \"\"\"Provide feedback on the contents of this exercise\n",
        "\n",
        "    Returns:\n",
        "        string\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
          "grade": true,
          "grade_id": "feedback-tests",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "GpWc2O2hx_aR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}